#Question Answer

1. Include a brief findings report comparing the effectiveness of different chunking strategies and embedding models based on retrieval accuracy and latency.

-The chunking method that I implemented was custom chunking as I defined the tokens for chunking with overlapping token count. This helps prevent the context gap between chunks.
-Token counting is done to prevent the context issue later on when the chunks are passed into the llm with chunks. 
-The latency of the custom chunking is faster than other chunking strategies like Semantic chunking and Recursive chunking.
-The semantic chunking chunks the text as seperated by paragraphs, sentences and any such boundaries.
-Simantic chunking has better performance but at the cost of latency. Hence, this technique avoided for chunking.

-SentenceTransformer model was used for embedding generation. The dimension of the embedding it generates is 384. Hence, the embedding generation using this model is faster than other embedding models.
-OpenAI models such as 'text-embedding-3-small' could also be used for embedding generation. But, due to its slower nature and price-applicable need, caused by API calls, it wasnt used.
-Embedding generated by text-embedding-3-small model is 1536 in dimension which makes it slower. However, it is rich in accuracy.


Compare two different similarity search algorithms supported by your selected vector DB and include findings on which worked better.

-The two search algorithmns supported by the vector DB I used was cosine similarity and dot product.
-Comparing between these two search algorithmns, Cosine similarity algorithm performed better than the dot product both on performance and latency.
-The cosine similarity produced more relevant results.
-Doot product may perform better with other embedding models, but it performed average and less efficient than the cosine algorithm in accuracy.